
#........[ TOY PARSER GENERATOR ].........................!
#                                                        ! !
# Warning: This file was automatically generated by TPG ! | !
# Do not edit this file unless you know what you do.   !  |  !
#                                                     !   @   !
#....................................................!!!!!!!!!!!
#
# For further information about TPG you can visit
# http://christophe.delord.free.fr/en/tpg

import base


"""
Toy Parser Generator: A Python parser generator
Copyright (C) 2002 Christophe Delord
 
This library is free software; you can redistribute it and/or
modify it under the terms of the GNU Lesser General Public
License as published by the Free Software Foundation; either
version 2.1 of the License, or (at your option) any later version.

This library is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
Lesser General Public License for more details.

You should have received a copy of the GNU Lesser General Public
License along with this library; if not, write to the Free Software
Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA

For further information about TPG you can visit
http://christophe.delord.free.fr/en/tpg
"""

from codegen import *

cut = lambda n: lambda s,n=n:s[n:-n]

class TPGParser(base.ToyParser,):

	def _init_scanner(self):
		self._lexer = base._Scanner(
			base._TokenDef(r"parser", r"parser"),
			base._TokenDef(r"_tok_1", r"\("),
			base._TokenDef(r"_tok_2", r"\)"),
			base._TokenDef(r"_tok_3", r":"),
			base._TokenDef(r"main", r"main"),
			base._TokenDef(r"set", r"set"),
			base._TokenDef(r"_tok_4", r"="),
			base._TokenDef(r"token", r"token"),
			base._TokenDef(r"separator", r"separator"),
			base._TokenDef(r"_tok_5", r";"),
			base._TokenDef(r"_tok_6", r","),
			base._TokenDef(r"_tok_7", r"<"),
			base._TokenDef(r"_tok_8", r">"),
			base._TokenDef(r"_tok_9", r"\.\."),
			base._TokenDef(r"_tok_10", r"\."),
			base._TokenDef(r"_tok_11", r"\["),
			base._TokenDef(r"_tok_12", r"\]"),
			base._TokenDef(r"_tok_13", r"->"),
			base._TokenDef(r"lex", r"lex"),
			base._TokenDef(r"_tok_14", r"\|"),
			base._TokenDef(r"check", r"check"),
			base._TokenDef(r"error", r"error"),
			base._TokenDef(r"_tok_15", r"-"),
			base._TokenDef(r"_tok_16", r"!"),
			base._TokenDef(r"_tok_17", r"\?"),
			base._TokenDef(r"_tok_18", r"\*"),
			base._TokenDef(r"_tok_19", r"\+"),
			base._TokenDef(r"space", r"\s+|#.*", None, 1),
			base._TokenDef(r"string", r"\"(\\.|[^\"\\]+)*\"|'(\\.|[^'\\]+)*'", cut(1), 0),
			base._TokenDef(r"code", r"\{\{(\}?[^\}]+)*\}\}", cut(2), 0),
			base._TokenDef(r"obra", r"\{", None, 0),
			base._TokenDef(r"cbra", r"\}", None, 0),
			base._TokenDef(r"retsplit", r"//", None, 0),
			base._TokenDef(r"ret", r"/", None, 0),
			base._TokenDef(r"ident", r"\w+", None, 0),
		)

	def START(self,):
		""" START -> PARSERS """
		parsers = self.PARSERS()
		return parsers.genCode()

	def PARSERS(self,):
		""" PARSERS -> OPTIONS (code)* ('parser' ident ('\(' ARGS '\)' | ) ':' (code | TOKEN | RULE | LEX_RULE)* | 'main' ':' (code)*)* """
		opts = self.OPTIONS()
		parsers = Parsers(opts)
		__p1 = self._cur_token
		while 1:
			try:
				c = self._eat('code')
				parsers.add(Code(c))
				__p1 = self._cur_token
			except self.TPGWrongMatch:
				self._cur_token = __p1
				break
		__p2 = self._cur_token
		while 1:
			try:
				try:
					self._eat('parser')
					id = self._eat('ident')
					__p3 = self._cur_token
					try:
						self._eat('_tok_1') # \(
						ids = self.ARGS()
						self._eat('_tok_2') # \)
					except self.TPGWrongMatch:
						self._cur_token = __p3
						ids = Args()
					self._eat('_tok_3') # :
					p = Parser(id,ids)
					__p4 = self._cur_token
					while 1:
						try:
							try:
								try:
									try:
										c = self._eat('code')
										p.add(Code(c))
									except self.TPGWrongMatch:
										self._cur_token = __p4
										t = self.TOKEN()
										p.add(t)
								except self.TPGWrongMatch:
									self._cur_token = __p4
									r = self.RULE()
									p.add(r)
							except self.TPGWrongMatch:
								self._cur_token = __p4
								r = self.LEX_RULE()
								p.add(r)
							__p4 = self._cur_token
						except self.TPGWrongMatch:
							self._cur_token = __p4
							break
					parsers.add(p)
				except self.TPGWrongMatch:
					self._cur_token = __p2
					self._eat('main')
					self._eat('_tok_3') # :
					__p5 = self._cur_token
					while 1:
						try:
							c = self._eat('code')
							parsers.add(Code(c))
							__p5 = self._cur_token
						except self.TPGWrongMatch:
							self._cur_token = __p5
							break
				__p2 = self._cur_token
			except self.TPGWrongMatch:
				self._cur_token = __p2
				break
		return parsers

	def OPTIONS(self,):
		""" OPTIONS -> ('set' ident ('=' string | ))* """
		opts = Options()
		self.CSL = 0
		__p1 = self._cur_token
		while 1:
			try:
				self._eat('set')
				opt = self._eat('ident')
				__p2 = self._cur_token
				try:
					self._eat('_tok_4') # =
					val = self._eat('string')
				except self.TPGWrongMatch:
					self._cur_token = __p2
					val = 1
				if opt.startswith('no'): opt, val = opt[2:], None 
				self.check(opt in [ 'magic', 'CSL' ])
				opts.set(opt,val) 
				if opt == 'CSL': self.CSL = val 
				__p1 = self._cur_token
			except self.TPGWrongMatch:
				self._cur_token = __p1
				break
		return opts

	def CHECK_CSL(self,):
		""" CHECK_CSL ->  |  """
		__p1 = self._cur_token
		try:
			self.check(self.CSL )
		except self.TPGWrongMatch:
			self._cur_token = __p1
			self.error(r"Only for CSL lexers")

	def CHECK_NOT_CSL(self,):
		""" CHECK_NOT_CSL ->  |  """
		__p1 = self._cur_token
		try:
			self.check(not self.CSL )
		except self.TPGWrongMatch:
			self._cur_token = __p1
			self.error(r"Only for non CSL lexers")

	def TOKEN(self,):
		""" TOKEN -> ('token' | 'separator') CHECK_NOT_CSL ident ':' string (OBJECT | ) ';' """
		__p1 = self._cur_token
		try:
			self._eat('token')
			s = 0
		except self.TPGWrongMatch:
			self._cur_token = __p1
			self._eat('separator')
			s = 1
		self.CHECK_NOT_CSL()
		t = self._eat('ident')
		self._eat('_tok_3') # :
		e = self._eat('string')
		__p2 = self._cur_token
		try:
			f = self.OBJECT()
		except self.TPGWrongMatch:
			self._cur_token = __p2
			f = None
		self._eat('_tok_5') # ;
		return Token(t,e,f,s)

	def ARGS(self,):
		""" ARGS -> (OBJECT (',' OBJECT)*)? """
		objs = Args()
		__p1 = self._cur_token
		try:
			obj = self.OBJECT()
			objs.add(obj)
			__p2 = self._cur_token
			while 1:
				try:
					self._eat('_tok_6') # ,
					obj = self.OBJECT()
					objs.add(obj)
					__p2 = self._cur_token
				except self.TPGWrongMatch:
					self._cur_token = __p2
					break
		except self.TPGWrongMatch:
			self._cur_token = __p1
		return objs

	def OBJECT(self,):
		""" OBJECT -> ident SOBJECT | string SOBJECT | '<' OBJECTS '>' | code """
		__p1 = self._cur_token
		try:
			try:
				try:
					o = self._eat('ident')
					o = self.SOBJECT(Object(o))
				except self.TPGWrongMatch:
					self._cur_token = __p1
					o = self._eat('string')
					o = self.SOBJECT(String(o))
			except self.TPGWrongMatch:
				self._cur_token = __p1
				self._eat('_tok_7') # <
				o = self.OBJECTS()
				self._eat('_tok_8') # >
		except self.TPGWrongMatch:
			self._cur_token = __p1
			c = self._eat('code')
			self.check(c.count('\n')==0)
			o = Code(c)
		return o

	def SOBJECT(self,o):
		""" SOBJECT -> ('\.\.' OBJECT | '\.' ident SOBJECT | '<' ARGS '>' SOBJECT | '\[' INDICE '\]' SOBJECT | ) """
		__p1 = self._cur_token
		try:
			try:
				try:
					try:
						self._eat('_tok_9') # \.\.
						o2 = self.OBJECT()
						o = Extraction(o,o2)
					except self.TPGWrongMatch:
						self._cur_token = __p1
						self._eat('_tok_10') # \.
						o2 = self._eat('ident')
						o = self.SOBJECT(Composition(o,Object(o2)))
				except self.TPGWrongMatch:
					self._cur_token = __p1
					self._eat('_tok_7') # <
					as = self.ARGS()
					self._eat('_tok_8') # >
					o = self.SOBJECT(Application(o,as))
			except self.TPGWrongMatch:
				self._cur_token = __p1
				self._eat('_tok_11') # \[
				i = self.INDICE()
				self._eat('_tok_12') # \]
				o = self.SOBJECT(Indexation(o,i))
		except self.TPGWrongMatch:
			self._cur_token = __p1
		return o

	def OBJECTS(self,):
		""" OBJECTS -> (OBJECT (',' OBJECT)*)? """
		objs = Objects()
		__p1 = self._cur_token
		try:
			obj = self.OBJECT()
			objs.add(obj)
			__p2 = self._cur_token
			while 1:
				try:
					self._eat('_tok_6') # ,
					obj = self.OBJECT()
					objs.add(obj)
					__p2 = self._cur_token
				except self.TPGWrongMatch:
					self._cur_token = __p2
					break
		except self.TPGWrongMatch:
			self._cur_token = __p1
		return objs

	def INDICE(self,):
		""" INDICE -> (OBJECT | ) (':' (OBJECT | ))? """
		__p1 = self._cur_token
		try:
			i = self.OBJECT()
		except self.TPGWrongMatch:
			self._cur_token = __p1
			i = None
		__p2 = self._cur_token
		try:
			self._eat('_tok_3') # :
			__p3 = self._cur_token
			try:
				i2 = self.OBJECT()
			except self.TPGWrongMatch:
				self._cur_token = __p3
				i2 = None
			i = Slice(i,i2)
		except self.TPGWrongMatch:
			self._cur_token = __p2
		self.check(i is not None)
		return i

	def RULE(self,):
		""" RULE -> SYMBOL '->' EXPR ';' """
		s = self.SYMBOL()
		self._eat('_tok_13') # ->
		e = self.EXPR()
		self._eat('_tok_5') # ;
		return Rule(s,e)

	def LEX_RULE(self,):
		""" LEX_RULE -> 'lex' CHECK_CSL ('separator' | SYMBOL) '->' EXPR ';' """
		self._eat('lex')
		self.CHECK_CSL()
		__p1 = self._cur_token
		try:
			name = self._eat('separator')
			s = Symbol(name,Args(),None)
		except self.TPGWrongMatch:
			self._cur_token = __p1
			s = self.SYMBOL()
		self._eat('_tok_13') # ->
		e = self.EXPR()
		self._eat('_tok_5') # ;
		return LexRule(s,e)

	def SYMBOL(self,):
		""" SYMBOL -> ident ('<' ARGS '>' | ) ('/' OBJECT | ) """
		id = self._eat('ident')
		__p1 = self._cur_token
		try:
			self._eat('_tok_7') # <
			as = self.ARGS()
			self._eat('_tok_8') # >
		except self.TPGWrongMatch:
			self._cur_token = __p1
			as = Args()
		__p2 = self._cur_token
		try:
			self._eat('ret') # /
			ret = self.OBJECT()
		except self.TPGWrongMatch:
			self._cur_token = __p2
			ret = None
		return Symbol(id,as,ret)

	def EXPR(self,):
		""" EXPR -> TERM ('\|' TERM)* """
		e = self.TERM()
		__p1 = self._cur_token
		while 1:
			try:
				self._eat('_tok_14') # \|
				t = self.TERM()
				e = Alternative(e,t)
				__p1 = self._cur_token
			except self.TPGWrongMatch:
				self._cur_token = __p1
				break
		return e

	def TERM(self,):
		""" TERM -> (FACT)* """
		t = Sequence()
		__p1 = self._cur_token
		while 1:
			try:
				f = self.FACT()
				t.add(f)
				__p1 = self._cur_token
			except self.TPGWrongMatch:
				self._cur_token = __p1
				break
		return t

	def FACT(self,):
		""" FACT -> AST_OP | MARK_OP | code | ATOM REP | 'check' OBJECT | 'error' OBJECT """
		__p1 = self._cur_token
		try:
			try:
				try:
					try:
						try:
							f = self.AST_OP()
						except self.TPGWrongMatch:
							self._cur_token = __p1
							f = self.MARK_OP()
					except self.TPGWrongMatch:
						self._cur_token = __p1
						c = self._eat('code')
						f = Code(c)
				except self.TPGWrongMatch:
					self._cur_token = __p1
					f = self.ATOM()
					f = self.REP(f)
			except self.TPGWrongMatch:
				self._cur_token = __p1
				self._eat('check')
				cond = self.OBJECT()
				f = Check(cond)
		except self.TPGWrongMatch:
			self._cur_token = __p1
			self._eat('error')
			err = self.OBJECT()
			f = Error(err)
		return f

	def AST_OP(self,):
		""" AST_OP -> OBJECT ('=' | '-') OBJECT """
		o1 = self.OBJECT()
		__p1 = self._cur_token
		try:
			self._eat('_tok_4') # =
			op = MakeAST
		except self.TPGWrongMatch:
			self._cur_token = __p1
			self._eat('_tok_15') # -
			op = AddAST
		o2 = self.OBJECT()
		return op(o1,o2)

	def MARK_OP(self,):
		""" MARK_OP -> '!' OBJECT """
		self._eat('_tok_16') # !
		o = self.OBJECT()
		return Mark(o)

	def ATOM(self,):
		""" ATOM -> (SYMBOL | INLINE_TOKEN | '\(' EXPR '\)') """
		__p1 = self._cur_token
		try:
			try:
				a = self.SYMBOL()
			except self.TPGWrongMatch:
				self._cur_token = __p1
				a = self.INLINE_TOKEN()
		except self.TPGWrongMatch:
			self._cur_token = __p1
			self._eat('_tok_1') # \(
			a = self.EXPR()
			self._eat('_tok_2') # \)
		return a

	def REP(self,a):
		""" REP -> ('\?' | '\*' | '\+' | '\{' NB (',' NB | ) '\}')? """
		__p1 = self._cur_token
		try:
			try:
				try:
					try:
						self._eat('_tok_17') # \?
						a = Rep(self,0,1,a)
					except self.TPGWrongMatch:
						self._cur_token = __p1
						self._eat('_tok_18') # \*
						a = Rep(self,0,None,a)
				except self.TPGWrongMatch:
					self._cur_token = __p1
					self._eat('_tok_19') # \+
					a = Rep(self,1,None,a)
			except self.TPGWrongMatch:
				self._cur_token = __p1
				self._eat('obra') # \{
				m = self.NB(0)
				__p2 = self._cur_token
				try:
					self._eat('_tok_6') # ,
					M = self.NB(None)
				except self.TPGWrongMatch:
					self._cur_token = __p2
					M = m
				self._eat('cbra') # \}
				a = Rep(self,m,M,a)
		except self.TPGWrongMatch:
			self._cur_token = __p1
		return a

	def NB(self,n):
		""" NB -> ident? """
		__p1 = self._cur_token
		try:
			n = self._eat('ident')
		except self.TPGWrongMatch:
			self._cur_token = __p1
		return n

	def INLINE_TOKEN(self,):
		""" INLINE_TOKEN -> string ('/' OBJECT | '//' CHECK_CSL OBJECT | ) """
		expr = self._eat('string')
		__p1 = self._cur_token
		try:
			try:
				self._eat('ret') # /
				ret = self.OBJECT()
				split = None
			except self.TPGWrongMatch:
				self._cur_token = __p1
				self._eat('retsplit') # //
				self.CHECK_CSL()
				ret = self.OBJECT()
				split = 1
		except self.TPGWrongMatch:
			self._cur_token = __p1
			ret = None
			split = None
		return InlineToken(expr,ret,split)



_TPGParser = TPGParser()

def compile(grammar):
	""" Translate a grammar into Python """
	return _TPGParser(grammar)
