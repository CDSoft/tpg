#!/usr/bin/env python

#........[ TOY PARSER GENERATOR ].........................!
#                                                        ! !
# Warning: This file was automatically generated by TPG ! | !
# Do not edit this file unless you know what you do.   !  |  !
#                                                     !   @   !
#....................................................!!!!!!!!!!!
#
# For further information about TPG you can visit
# http://christophe.delord.free.fr/en/tpg


from __future__ import generators
import re


class _TokenDef:
	""" Token definition for the scanner """

	ident_pat = re.compile(r'^\w+$')
	
	def __init__(self, tok, regex=None, action=None, separator=0):
		if regex is None: regex = tok
		if self.ident_pat.match(regex): regex += r'\b'	# match 'if\b' instead of 'if'
		if action is None: action = lambda x:x			# default action if identity
		elif not callable(action): action = lambda x,y=action:y	# action must be callable
		self.tok = tok							# token name
		self.regex = '(?P<%s>%s)'%(tok, regex)	# token regexp
		self.action = action					# token modifier
		self.separator = separator				# is this a separator ?

	def __str__(self):
		return "token %s: %s %s;"%(self.tok, self.regex, self.action)

class _Token:
	""" Token instanciated while scanning """

	def __init__(self, tok, text, val, lineno, start, end):
		self.tok = tok			# token type
		self.text = text		# matched text
		self.val = val			# value (ie action(text))
		self.lineno = lineno	# token line number
		self.start = start		# token start index
		self.end = end			# token end index

	def __str__(self):
		return "%d:%s[%s]"%(self.lineno, self.tok, self.val)

class _Eof:
	""" EOF token """

	def __init__(self, lineno='EOF'):
		self.lineno = lineno
		self.text = 'EOF'
		self.val = 'EOF'

	def __str__(self):
		return "%s:Eof"%self.lineno

class _Scanner:
	""" Lexical scanner """

	def __init__(self, *tokens):
		regex = []				# regex list
		actions = {}			# dict token->action
		separator = {}			# set of separators
		for token in tokens:
			regex.append(token.regex)
			actions[token.tok] = token.action
			separator[token.tok] = token.separator
		self.regex = re.compile('|'.join(regex))	# regex is the choice between all tokens
		self.actions = actions
		self.separator = separator

	def tokens(self, input):
		""" Scan input and return a list of _Token instances """
		self.input = input
		i = 0				# start of the next token
		l = len(input)
		lineno = 1			# current token line number
		toks = []			# token list
		while i<l:												# while not EOF
			token = self.regex.match(input,i)					# get next token
			if not token:										# if none raise LexicalError
				last = toks and toks[-1] or _Eof(lineno)
				raise LexicalError(last)
			j = token.end()										# end of the current token
			for (t,v) in token.groupdict().items():				# search the matched token
				if v is not None and self.actions.has_key(t):
					tok = t										# get its type
					text = token.group()						# get matched text
					val = self.actions[tok](text)				# compute its value
					break
			if not self.separator[tok]:								# if the matched token is a real token
				toks.append(_Token(tok, text, val, lineno, i, j))	# store it
			lineno += input.count('\n', i, j)					# update lineno
			i = j												# go to the start of the next token
		return toks

class TPGWrongMatch(Exception):
	def __init__(self, last):
		self.last = last

class LexicalError(Exception):
	def __init__(self, last):
		self.last = last
	def __str__(self):
		if self.last:
			return "%s: Lexical error near %s"%(self.last.lineno, self.last.text)
		else:
			return "1: Lexical error"

class SyntaxError(Exception):
	def __init__(self, last):
		self.last = last
	def __str__(self):
		if self.last:
			return "%s: Syntax error near %s"%(self.last.lineno, self.last.text)
		else:
			return "1: Syntax error"

class ToyParser:
	""" Base class for every TPG parsers """

	def __init__(self):
		self._init_scanner()

	def _eat(self, token):
		""" Eat one token """
		try:
			t = self._tokens[self._cur_token]	# get current token
		except IndexError:						# if EOF
			self.WrongMatch()					# raise TPGWrongMatch to backtrack
		if t.tok == token:						# if current is an expected token
			self._cur_token += 1				# go to the next one
			return t.val						# and return its value
		else:
			self.WrongMatch()					# else backtrack

	def WrongMatch(self):
		""" Backtracking """
		try:
			raise TPGWrongMatch(self._tokens[self._cur_token])
		except IndexError:
			raise TPGWrongMatch(_Eof())

	def check(self, cond):
		""" Check a condition while parsing """
		if not cond:			# if condition is false
			self.WrongMatch()	# backtrack

	def __call__(self, input, *args):
		""" Parse the axiom of the grammar (if any) """
		return self.parse('START', input, *args)

	def parse(self, symbol, input, *args):
		""" Parse an input start at a given symbol """
		try:
			self._tokens = self._lexer.tokens(input)	# scan tokens
			self._cur_token = 0							# start at the first token
			ret = getattr(self, symbol)(*args)			# call the symbol
			if self._cur_token < len(self._tokens):		# if there are unparsed tokens
				self.WrongMatch()						# raise an error
			return ret									# otherwise return the result
		except TPGWrongMatch, e:					# convert an internal TPG error
			raise SyntaxError(e.last)				# into a SyntaxError

	def _mark(self):
		""" Get a mark for the current token """
		return self._cur_token

	def _extract(self, a, b):
		""" Extract text between 2 marks """
		if not self._tokens: return ""
		if a<len(self._tokens):
			start = self._tokens[a].start
		else:
			start = self._tokens[-1].end
		if b>0:
			end = self._tokens[b-1].end
		else:
			end = self._tokens[0].start
		return self._lexer.input[start:end]

	def lineno(self, mark=None):
		""" Get the line number of a mark (or the current token if none) """
		if mark is None: mark = self._cur_token
		if not self._tokens: return 0
		if mark<len(self._tokens):
			return self._tokens[mark].lineno
		else:
			return self._tokens[-1].lineno
	


from string import atoi, atof, atol
from math import sqrt, cos, sin, tan, acos, asin, atan
class Calc(ToyParser,dict):

	def _init_scanner(self):
		self._lexer = _Scanner(
			_TokenDef(r"vars", r"vars"),
			_TokenDef(r"_tok_1", r"="),
			_TokenDef(r"_tok_2", r"\("),
			_TokenDef(r"_tok_3", r"\)"),
			_TokenDef(r"_tok_4", r","),
			_TokenDef(r"space", r"(\s|\n)+", None, 1),
			_TokenDef(r"pow_op", r"\^|\*\*", self.make_op, 0),
			_TokenDef(r"add_op", r"[+-]", self.make_op, 0),
			_TokenDef(r"mul_op", r"[*/%]", self.make_op, 0),
			_TokenDef(r"funct1", r"(cos|sin|tan|acos|asin|atan|sqr|sqrt|abs)\b", self.make_op, 0),
			_TokenDef(r"funct2", r"(norm)\b", self.make_op, 0),
			_TokenDef(r"real", r"(\d+\.\d*|\d*\.\d+)([eE][-+]?\d+)?|\d+[eE][-+]?\d+", atof, 0),
			_TokenDef(r"integer", r"\d+", atol, 0),
			_TokenDef(r"VarId", r"[a-zA-Z_]\w*", None, 0),
		)

	
	def mem(self):
		vars = self.items()
		vars.sort()
		return "\n\t" + "\n\t".join([ "%s = %s"%(var, val) for (var, val) in vars ])
	
	def make_op(self, op):
		return {
			'+'   : (lambda x,y:x+y),
			'-'   : (lambda x,y:x-y),
			'*'   : (lambda x,y:x*y),
			'/'   : (lambda x,y:x/y),
			'%'   : (lambda x,y:x%y),
			'^'   : (lambda x,y:x**y),
			'**'  : (lambda x,y:x**y),
			'cos' : cos,
			'sin' : sin,
			'tan' : tan,
			'acos': acos,
			'asin': asin,
			'atan': atan,
			'sqr' : (lambda x:x*x),
			'sqrt': sqrt,
			'abs' : abs,
			'norm': (lambda x,y:sqrt(x*x+y*y)),
		}[op]
	def START(self,):
		""" START -> 'vars' | VarId '=' Expr | Expr """
		__p1 = self._cur_token
		try:
			try:
				self._eat('vars')
				e = self.mem()
			except TPGWrongMatch:
				self._cur_token = __p1
				v = self._eat('VarId')
				self._eat('_tok_1') # =
				e = self.Expr()
				self[v] = e
		except TPGWrongMatch:
			self._cur_token = __p1
			e = self.Expr()
		return e

	def Var(self,):
		""" Var -> VarId """
		v = self._eat('VarId')
		return self.get(v,0)

	def Expr(self,):
		""" Expr -> Term (add_op Term)* """
		e = self.Term()
		__p1 = self._cur_token
		while 1:
			try:
				op = self._eat('add_op')
				t = self.Term()
				e = op(e,t)
				__p1 = self._cur_token
			except TPGWrongMatch:
				self._cur_token = __p1
				break
		return e

	def Term(self,):
		""" Term -> Fact (mul_op Fact)* """
		t = self.Fact()
		__p1 = self._cur_token
		while 1:
			try:
				op = self._eat('mul_op')
				f = self.Fact()
				t = op(t,f)
				__p1 = self._cur_token
			except TPGWrongMatch:
				self._cur_token = __p1
				break
		return t

	def Fact(self,):
		""" Fact -> add_op Fact | Pow """
		__p1 = self._cur_token
		try:
			op = self._eat('add_op')
			f = self.Fact()
			f = op(0,f)
		except TPGWrongMatch:
			self._cur_token = __p1
			f = self.Pow()
		return f

	def Pow(self,):
		""" Pow -> Atom (pow_op Fact)? """
		f = self.Atom()
		__p1 = self._cur_token
		try:
			op = self._eat('pow_op')
			e = self.Fact()
			f = op(f,e)
		except TPGWrongMatch:
			self._cur_token = __p1
		return f

	def Atom(self,):
		""" Atom -> real | integer | Function | Var | '\(' Expr '\)' """
		__p1 = self._cur_token
		try:
			try:
				try:
					try:
						a = self._eat('real')
					except TPGWrongMatch:
						self._cur_token = __p1
						a = self._eat('integer')
				except TPGWrongMatch:
					self._cur_token = __p1
					a = self.Function()
			except TPGWrongMatch:
				self._cur_token = __p1
				a = self.Var()
		except TPGWrongMatch:
			self._cur_token = __p1
			self._eat('_tok_2') # \(
			a = self.Expr()
			self._eat('_tok_3') # \)
		return a

	def Function(self,):
		""" Function -> funct1 '\(' Expr '\)' | funct2 '\(' Expr ',' Expr '\)' """
		__p1 = self._cur_token
		try:
			f = self._eat('funct1')
			self._eat('_tok_2') # \(
			x = self.Expr()
			self._eat('_tok_3') # \)
			y = f(x)
		except TPGWrongMatch:
			self._cur_token = __p1
			f = self._eat('funct2')
			self._eat('_tok_2') # \(
			x1 = self.Expr()
			self._eat('_tok_4') # ,
			x2 = self.Expr()
			self._eat('_tok_3') # \)
			y = f(x,y)
		return y


calc = Calc()
while 1:
	l = raw_input("\n:")
	if l:
		try:
			print calc(l)
		except (SyntaxError, LexicalError), e:
			print e
		except ZeroDivisionError:
			print "Zero Division Error"
		except OverflowError:
			print "Overflow Error"
	else:
		break